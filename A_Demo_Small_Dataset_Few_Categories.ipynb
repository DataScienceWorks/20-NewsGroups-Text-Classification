{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ref.: http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = ['alt.atheism', 'soc.religion.christian', 'comp.graphics', 'sci.med']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "twenty_train = fetch_20newsgroups(subset='train', categories=categories, shuffle=True, random_state=42) \n",
    "# Output: The returned dataset is a scikit-learn “bunch”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Categories:')\n",
    "for i, cat in enumerate(twenty_train.target_names):\n",
    "    print(i,' : ',cat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(twenty_train.data)) # 2257\n",
    "print(len(twenty_train.filenames)) # 2257"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('################ Data ############')\n",
    "print(\"\\n\".join(twenty_train.data[0].split(\"\\n\")[:40]))\n",
    "\n",
    "print('################ Target ############')\n",
    "print(twenty_train.target_names[twenty_train.target[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For speed and space efficiency reasons scikit-learn loads the target attribute as an array of integers that corresponds to the index of the category name in the target_names list. The category integer id of each sample is stored in the target attribute:\n",
    "twenty_train.target[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# It is possible to get back the category names as follows:\n",
    "m = map(lambda x: twenty_train.target_names[x], twenty_train.target[:10])\n",
    "[*m]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting features from text files\n",
    "Ref.: http://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html#extracting-features-from-text-files\n",
    "\n",
    "In order to perform machine learning on text documents, we first need to turn the text content into numerical feature vectors.\n",
    "\n",
    "### Bag  of Words\n",
    "The bags of words representation implies that `n_features` is the number of distinct words in the corpus: this number is typically larger than 100,000.\n",
    "\n",
    "If `n_samples == 10000`, storing X as a numpy array of type `float32` would require `10000 x 100000 x 4 bytes = 4GB` in RAM.\n",
    "\n",
    "Fortunately, most values in X will be zeros since for a given document less than a couple thousands of distinct words will be used. For this reason we say that bags of words are typically high-dimensional sparse datasets. We can save a lot of memory by only storing the non-zero parts of the feature vectors in memory.\n",
    "\n",
    "`scipy.sparse` matrices are data structures that do exactly this, and `scikit-learn` has built-in support for these structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "count_vect = CountVectorizer()\n",
    "\n",
    "xtrain_counts = count_vect.fit_transform(twenty_train.data)\n",
    "xtrain_counts.shape # (2257, 35788)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CountVectorizer supports counts of N-grams of words or consecutive characters. \n",
    "# Once fitted, the vectorizer has built a dictionary of feature indices\n",
    "count_vect.vocabulary_.get(u'game') # 15077\n",
    "# The index value of a word in the vocabulary is linked to its frequency in the whole training corpus."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From Occurrancecs to Frequencies\n",
    "Occurrence count is a good start but there is an issue: longer documents will have higher average count values than shorter documents, even though they might talk about the same topics.\n",
    "\n",
    "To avoid these potential discrepancies it suffices to divide the number of occurrences of each word in a document by the total number of words in the document: these new features are called `tf` for Term Frequencies.\n",
    "\n",
    "Another refinement on top of tf is to downscale weights for words that occur in many documents in the corpus and are therefore less informative than those that occur only in a smaller portion of the corpus.\n",
    "\n",
    "This downscaling is called **tf–idf** for **“Term Frequency times Inverse Document Frequency”**.\n",
    "\n",
    "Both `tf` and `tf–idf` can be computed as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "# tf_transformer = TfidfTransformer(use_idf=False)\n",
    "# xtrain_tf = tf_transformer.fit_transform(xtrain_counts)\n",
    "\n",
    "tfidf_transformer = TfidfTransformer()\n",
    "xtrain_tfidf = tfidf_transformer.fit_transform(xtrain_counts)\n",
    "xtrain_tfidf.shape # (2257, 35788)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a classifier\n",
    "\n",
    "Now that we have our features, we can train a classifier to try to predict the category of a post.\n",
    "\n",
    "Let’s start with a naïve Bayes classifier, which provides a nice baseline for this task.\n",
    "\n",
    "Note:  `scikit-learn` includes several variants of this classifier; the one most suitable for word counts is the multinomial variant:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB().fit(xtrain_tfidf, twenty_train.target)\n",
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To try to predict the outcome on a new document we need to extract the features using almost the same feature extracting chain as before.\n",
    "docs_new = ['God is love', 'OpenGL on the GPU is fast', 'I am God loving if not God fearing', 'I am the prince of Asia']\n",
    "xnew_counts = count_vect.transform(docs_new)\n",
    "xnew_tfidf = tfidf_transformer.transform(xnew_counts)\n",
    "\n",
    "preds = clf.predict(xnew_tfidf)\n",
    "print(preds)\n",
    "for doc,pred in zip(docs_new, preds):\n",
    "    cat = twenty_train.target_names[pred] # Converting numeric category to string\n",
    "    print('%r => %s' % (doc, cat))\n",
    "'''\n",
    "[3 1 3 2]\n",
    "'God is love' => soc.religion.christian\n",
    "'OpenGL on the GPU is fast' => comp.graphics\n",
    "'I am God loving if not God fearing' => soc.religion.christian\n",
    "'I am the prince of Asia' => sci.med\n",
    "'''    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a pipeline\n",
    "In order to make the vectorizer => transformer => classifier easier to work with, `scikit-learn` provides a `Pipeline` class that behaves like a compound classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "text_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', MultinomialNB()),\n",
    "])\n",
    "\n",
    "# The names vect, tfidf and clf (classifier) are arbitrary. \n",
    "# We shall see their use in the section on grid search, below. \n",
    "# We can now train the model with a single command:\n",
    "text_clf.fit(twenty_train.data, twenty_train.target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of the performance on the test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "twenty_test = fetch_20newsgroups(subset='test', categories=categories, shuffle=True, random_state=42)\n",
    "preds = text_clf.predict(twenty_test.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "acc = np.mean(preds == twenty_test.target) # 0.8348868175765646\n",
    "print('Accuracy = ', acc)\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "creport = classification_report(twenty_test.target, preds, target_names=twenty_test.target_names)\n",
    "print(creport)\n",
    "\n",
    "confusion_matrix(twenty_test.target, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of the performance on the test set using SVM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "svm_clf = Pipeline([('vect', CountVectorizer()),\n",
    "                     ('tfidf', TfidfTransformer()),\n",
    "                     ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                                           alpha=1e-3, random_state=42,\n",
    "                                           max_iter=5, tol=None)),\n",
    "])\n",
    "svm_clf.fit(twenty_train.data, twenty_train.target)  \n",
    "\n",
    "preds = svm_clf.predict(twenty_test.data)\n",
    "acc = np.mean(preds == twenty_test.target)\n",
    "print('Accuracy = ', acc) # 0.9127829560585885\n",
    "\n",
    "creport = classification_report(twenty_test.target, preds, target_names=twenty_test.target_names)\n",
    "print(creport)\n",
    "confusion_matrix(twenty_test.target, preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameter tuning using grid search\n",
    "\n",
    "Instead of tweaking the parameters of the various components of the chain, it is possible to run an exhaustive search of the best parameters on a grid of possible values. \n",
    "\n",
    "We try out all classifiers on either words or bigrams, with or without idf, and with a penalty parameter of either 0.01 or 0.001 for the linear SVM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
    "              'tfidf__use_idf': (True, False),\n",
    "              'clf__alpha': (1e-2, 1e-3),\n",
    "             }\n",
    "gs_clf = GridSearchCV(svm_clf, parameters, n_jobs=-1)\n",
    "\n",
    "# Let’s perform the search on a smaller subset of the training data to speed up the computation:\n",
    "gs_clf = gs_clf.fit(twenty_train.data[:400], twenty_train.target[:400]) # The data is shuffled, so we are good\n",
    "\n",
    "# The object’s best_score_ and best_params_ attributes store the best mean score and \n",
    "# the parameters setting corresponding to that score:\n",
    "print('Best Score : ', gs_clf.best_score_)\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))\n",
    "'''\n",
    "Best Score :  0.9\n",
    "clf__alpha: 0.001\n",
    "tfidf__use_idf: True\n",
    "vect__ngram_range: (1, 1)\n",
    "'''    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = gs_clf.predict(twenty_test.data)\n",
    "acc = np.mean(preds == twenty_test.target)\n",
    "print('Accuracy = ', acc) # 0.859520639147803\n",
    "\n",
    "creport = classification_report(twenty_test.target, preds, target_names=twenty_test.target_names)\n",
    "print(creport)\n",
    "confusion_matrix(twenty_test.target, preds)\n",
    "'''\n",
    "Accuracy =  0.859520639147803\n",
    "                        precision    recall  f1-score   support\n",
    "\n",
    "           alt.atheism       0.93      0.71      0.80       319\n",
    "         comp.graphics       0.84      0.96      0.90       389\n",
    "               sci.med       0.91      0.83      0.87       396\n",
    "soc.religion.christian       0.80      0.91      0.85       398\n",
    "\n",
    "           avg / total       0.87      0.86      0.86      1502\n",
    "\n",
    "array([[225,  12,  14,  68],\n",
    "       [  2, 373,   8,   6],\n",
    "       [  5,  45, 329,  17],\n",
    "       [  9,  13,  12, 364]], dtype=int64)\n",
    "'''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
